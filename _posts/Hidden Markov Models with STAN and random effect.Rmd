---
layout: post
title: "Hidden Markov Models with STAN"
author: "Rasim M Musal"
date: "2023-10-14"
output:
  html_document:
   theme: darkly
   highlight: espresso
   toc: true
   keep_md: yes
   toc_float: true
   toc_collapsed: true
   toc_depth: 3
   number_sections: true
   usemathjax: true
tags: [rshiny, maps,SMR]
always_allow_html: true
---
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: {
 
            autoNumber: "all",
            formatNumber: function (n) {return +n}
      } 
  }
});
</script>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE, message = FALSE,fig.path = ('./assets/img/2023-10-14-Hidden_Markov_Models_with_STAN'))
```

We had already discussed the [Hidden Markov Models](https://mmusal.github.io/blog/2023/Hidden-Markov-Models/). In this post we will discuss the details of the STAN model as well as the details we skipped in the above post.

We make use of two main sources and apply it to our context.\

1) Luis Damiano https://luisdamiano.github.io/stancon18/hmm_stan_tutorial.pdf\

2) Jim Savage https://khakieconomics.github.io/2018/02/24/Regime-switching-models.html\


Load the libraries, and read in the [SMR data](https://mmusal.github.io/blog/2023/Explaining_rshinyapp/#SMR). 
```{r}
library(ggplot2)
library(tidyr)
library(dplyr)
#E is calculated at every biweek and is the expected number of deaths if
#mortality was uniformly distributed across the population in the state of California
SMR=read.csv(file='SMR.csv',header = TRUE)[,-1]
#First 6 rows
head(SMR)

#Subset the data using dplyr
SMRSubset=SMR %>% filter(County %in% 
c("Alameda","Lassen","Kern","Los Angeles",
"Lassen","San Francisco"))

#Visualize 6 counties of interest.
ggplot(data=SMRSubset,aes(x=Time,y=SMR,color=County))+
geom_line()+  
  theme(legend.position = "right")+
  theme(axis.title.y =element_blank())+
  scale_x_discrete(limits=c(1,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,77))+
  xlab("Biweek t")+
  ggtitle("SMR Values of 6 Counties")+
  theme(plot.title = element_text(hjust = 0.5))


```

In this snippet we provide the data structure for compilation by STAN.
```{stan,echo=FALSE,output.var="model",eval=FALSE }
data { 
  int<lower=1> T;//number of time periods 77
  int K; //This will be fixed to 2
  int<lower=0> N;//number of spatial areas 58 counties in CA
  int<lower=0>  y[T,N]; //observed covid
  matrix [T,N] log_E; // The logarithm of E_{ti} 
}
```

The parameters of the model is defined here including the priors. 
```{stan,output.var="model",eval=FALSE }
parameters {
ordered[K] mu[N]; 
```
We will have 2 mu values for each County such that mu_{i,1}<mu_{i,2}, the ordering is neccessary for the identifiability of the parameters. Othwerise the distribution of $\mu$ would create problems of convergence. 

```{stan,echo=FALSE,output.var="model",eval=FALSE }
  simplex[K] pi1[1]; //pi1 is the prior distribution on K regimes common to all the counties. 
  simplex[K] A[K,N]; //transition probability matrix of N counties.

  real epsilon[T];//biweekly random effect common to all counties.
}
```

This is where we define the steps to calculate the joint probabilities note the dimensions of the objects. Logalpha contains joint probabilities of data and regime.
```{stan,output.var="model",eval=FALSE }
transformed parameters {
matrix[K,N] lambda [T] ;
real logalpha[T,K,N];
real accumulator[K];
    for(t in 1:T){
  for(k in 1:K){     
    for(n in 1:N){
      lambda[t][k,n]=exp( //Covid-19 mean and variance of k'th regime n'th county at time t.
        log_E[t,n]+mu[n,k]+epsilon[t]); //since lambda is the exponential of the linear equation lambda is  E[t,n]*exp(mu[n,k]+epsilon[t])
                }
              }
                }
for(k in 1:K){
  for(n in 1:N){
logalpha[1,k,n] = log(pi1[1,k]) + poisson_lpmf(y[1,n] | lambda[1,k,n]);
}
}
```
lambda contains the information of regime but also other information
we will ignore that notation for brevity
$logalpha_{1,k=1,n}=log(P(k_{1})=1)*P(y_{1,n}|\lambda_{1,1,n})$
$=P(y_{1,n},k_{1}=1)$
$logalpha_{1,k=2,n}=log(P(k_{1})=2)*P(y_{1,n}|\lambda{1,2,n})$
$=P(y_{1,n},k_{1}=2)$

Continuing with the transformed parameters.
Let us focus on an iteration at t=2, j=1 within the loop for n
```{stan,output.var="model",eval=FALSE }
for (t in 2:T) {
for (j in 1:K) {
for (n in 1:N){
accumulator[1] = logalpha[t-1, 1,n] + log(A[1, n,j]) + poisson_lpmf(y[t,n] | lambda[t,j,n]);
```

This is evaluating:
$accumulator_{1}=$\
$log(P(y_{1,n},k_{1}=1)*P(k_{2}=1|k_{1}=1)*P(y_{2,n}|\lambda_{2,1,n}))$\
This is equivalent to \
$log(P(y_{1,n},k_{1}=1,k_{2}=1)*P(y_{2,n}|\lambda_{2,1,n}))$\
$P(y_{1,n},k_{1}=1,k_{2}=1,y_{2,n})$\

```{stan,output.var="model",eval=FALSE }
accumulator[2] = logalpha[t-1, 2,n]+log(A[2, n,j])+poisson_lpmf(y[t,n] |lambda[t,j,n]);
```
using the same set of operations we calculate the other other joint probability.\
$accumulator_{2}=$\
$log(P(y_{1,n},k_{1}=2,k_{2}=1,y_{2,n}))$\


```{stan,output.var="model",eval=FALSE }
logalpha[t, j,n] = log_sum_exp(to_vector(accumulator))
}
}
}
;
```

Once you take the exp and sum the operations that occur:\
$P(y_{1,n},k_{1}=1,k_{2}=1,y_{2,n})+$\
$P(y_{1,n},k_{1}=2,k_{2}=1,y_{2,n})$\
and therefore you obtain in logalpha[2, 1,n]\
$=P(y_{1,n},k_{2}=1,y_{2,n})$
After j=1 is complete and when j=2 and t is still at 2
$=P(y_{1,n},k_{2}=2,y_{2,n})$
so in general at time T
$P(y_{1,n},...,y_{T,n},k_{T}=1)$ and 
$P(y_{1,n},...,y_{T,n},k_{T}=2)$

```{stan,output.var="model",eval=FALSE }
model {

for(n in 1:N){  
 target += log_sum_exp(to_vector(logalpha[T,1:K,n]));
```
This is going to evaluate the last joint probability
the rest being evaluated in the transformed parameters.
According to Luis Damio's conference paper the above is enough
to compute the likelihood I am testing it out right now
$P(y_{1,n},...,y_{T,n},k_{T}=1)$ and \
$P(y_{1,n},...,y_{T,n},k_{T}=2)$\
$log(P(y_{1,n},...,y_{T,n},k_{T}=1)+P(y_{1,n},...,y_{T,n},k_{T}=2))$\
$log(P(y_{1,n},...,y_{T,n}))$\


The priors are listed below

```{stan,output.var="model",eval=FALSE }
  mu[n]~normal(0,3);

}

epsilon~normal(0,3);
}

```

The priors given by simplex is Dirichlet [1,1]
https://discourse.mc-stan.org/t/default-prior-for-simplex/9748
ordered parameters are discussed
https://betanalpha.github.io/assets/case_studies/identifying_mixture_models.html#43_breaking_the_labeling_degeneracy_by_enforcing_an_ordering
log likelihood is listed on table 1
https://luisdamiano.github.io/stancon18/hmm_stan_tutorial.pdf
